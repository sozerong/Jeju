{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1WUIprDTP_GU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob, os, re, jieba\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model, save_model, load_model\n",
        "from keras.layers import Input, LSTM, Dense, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdjRztPwP_GW"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.python.client import device_lib\n",
        "#print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "dv30ka3iP_GX",
        "outputId": "316c2a79-4937-4903-9e4c-77bc640b61ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"gpus = tf.config.list_physical_devices('GPU')\\nif gpus:\\n    try:\\n        tf.config.experimental.set_memory_growth(gpus[0], True)\\n        print('GPU Running')\\n    except RuntimeError as e:\\n        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\\n        (e)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "'''gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print('GPU Running')\n",
        "    except RuntimeError as e:\n",
        "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
        "        (e)'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NotM6SXxREYJ",
        "outputId": "91b95eb0-a5d0-423d-9daf-bcdd64f30ca1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z5XoGhFkP_GX"
      },
      "outputs": [],
      "source": [
        "def preprocess_kr(w):\n",
        "    w = re.sub(r\"([?'!¿\\-·\\\"])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', r\" \", w)\n",
        "    w = re.sub(r\"\\,(?=[0-9])\", r\"\", w)\n",
        "    w = w[:-1].strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4wd8W7lMP_GX"
      },
      "outputs": [],
      "source": [
        "def extract_data_from_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    standard_forms = []\n",
        "    dialect_forms = []\n",
        "\n",
        "    for utterance in data['utterance']:\n",
        "        standard_forms.append(utterance['standard_form'])\n",
        "        dialect_forms.append(utterance['dialect_form'])\n",
        "\n",
        "    return standard_forms, dialect_forms\n",
        "\n",
        "def preprocess(path, num_data):\n",
        "    files = glob.glob(os.path.join(path, '*.json'))\n",
        "    std, jej = [], []\n",
        "\n",
        "    for f in files:\n",
        "        std_forms, dial_forms = extract_data_from_json(f)\n",
        "        std.extend(std_forms)\n",
        "        jej.extend(dial_forms)\n",
        "\n",
        "    std_series = pd.Series(std)\n",
        "    jej_series = pd.Series(jej)\n",
        "\n",
        "    df = pd.concat([std_series, jej_series], axis=1)\n",
        "    df.columns = ['표준어', '제주어']\n",
        "\n",
        "    df['표준어'] = df['표준어'].apply(preprocess_kr)\n",
        "    df['제주어'] = df['제주어'].apply(preprocess_kr)\n",
        "\n",
        "    df = df.sample(num_data, random_state=2)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RbOxtQdP_GY",
        "outputId": "dee35edc-be8a-4536-fbcb-7b5f32cf12ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표준어 tensor 최장 길이 : 34\n",
            "제주어 tensor 최장 길이 : 34\n"
          ]
        }
      ],
      "source": [
        "def tokenize(texts):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    tensor = tokenizer.texts_to_sequences(texts)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "    return tensor, tokenizer\n",
        "\n",
        "def tokenize_dataset(path, num_data):\n",
        "    df = preprocess(path, num_data)\n",
        "\n",
        "    std_tensor, std_tokenizer = tokenize(df['표준어'].values)\n",
        "    jej_tensor, jej_tokenizer = tokenize(df['제주어'].values)\n",
        "\n",
        "    return std_tensor, jej_tensor, std_tokenizer, jej_tokenizer\n",
        "\n",
        "# 설정한 경로와 데이터 크기 제한\n",
        "num_data = 15000\n",
        "path = '/content/drive/MyDrive/colab/Jeju/Training'\n",
        "std_tensor, jej_tensor, std_lang, jej_lang = tokenize_dataset(path, num_data)\n",
        "\n",
        "max_length_std = std_tensor.shape[1]\n",
        "max_length_jej = jej_tensor.shape[1]\n",
        "\n",
        "print('표준어 tensor 최장 길이 : {}'.format(max_length_std))\n",
        "print('제주어 tensor 최장 길이 : {}'.format(max_length_jej))\n",
        "\n",
        "std_tensor_train, std_tensor_val, jej_tensor_train, jej_tensor_val = train_test_split(std_tensor, jej_tensor, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mojO71kwQsUt",
        "outputId": "7e62385f-620d-446e-893e-978e0657c462"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/colab/Jeju/Training'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5q-y55iP_GY",
        "outputId": "3d065a72-d7ea-4abc-eafb-f45a5e89dffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표준어 tensor 최장 길이 : 34\n",
            "제주어 tensor 최장 길이 : 34\n"
          ]
        }
      ],
      "source": [
        "# 약 5분 소요\n",
        "std_tensor, jej_tensor, std_lang, jej_lang = tokenize_dataset(path, num_data)\n",
        "\n",
        "# 입력 텐서와 타겟 텐서의 최대 길이 계산\n",
        "max_length_std = std_tensor.shape[1]\n",
        "max_length_jej = jej_tensor.shape[1]\n",
        "\n",
        "print('표준어 tensor 최장 길이 : {}'.format(max_length_std))\n",
        "print('제주어 tensor 최장 길이 : {}'.format(max_length_jej))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUs6MUqOP_GZ",
        "outputId": "ec79852a-f5ba-4ca3-dffe-9333db45d9a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터셋 크기 : 12000, 12000\n",
            "검증 데이터셋 크기 : 3000, 3000\n"
          ]
        }
      ],
      "source": [
        "# 표준어와 제주어 텐서를 훈련 데이터셋과 검증 데이터셋으로 나누기\n",
        "std_tensor_train, std_tensor_val, jej_tensor_train, jej_tensor_val = train_test_split(std_tensor, jej_tensor, test_size=0.2)\n",
        "\n",
        "print('훈련 데이터셋 크기 : {}, {}'.format(len(std_tensor_train), len(jej_tensor_train)))\n",
        "print('검증 데이터셋 크기 : {}, {}'.format(len(std_tensor_val), len(jej_tensor_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFm9Y7s6P_GZ",
        "outputId": "b3465b1f-1524-4207-a551-5657bd8649a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표준어 index ----> token\n",
            "           1 ----> <start>\n",
            "          37 ----> 응\n",
            "       22528 ----> 초를\n",
            "         409 ----> 처음에\n",
            "       22529 ----> 쓰던\n",
            "       22530 ----> 초는\n",
            "          18 ----> 그렇게\n",
            "           8 ----> 안\n",
            "         160 ----> 했는\n",
            "           2 ----> <end>\n",
            "\n",
            "제주어 index ----> token\n",
            "           1 ----> <start>\n",
            "          36 ----> 응\n",
            "       24174 ----> 초를\n",
            "         400 ----> 처음에\n",
            "       24175 ----> 쓰던\n",
            "       24176 ----> 초는\n",
            "          95 ----> 경\n",
            "           8 ----> 안\n",
            "         322 ----> 해신\n",
            "           2 ----> <end>\n"
          ]
        }
      ],
      "source": [
        "def convert(tokenizer, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print(\"%12d ----> %s\" % (t, tokenizer.index_word[t]))\n",
        "\n",
        "print('표준어 index ----> token')\n",
        "convert(std_lang, std_tensor_train[0])\n",
        "print()\n",
        "print('제주어 index ----> token')\n",
        "convert(jej_lang, jej_tensor_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ANKqDVdP_GZ",
        "outputId": "5858c0f2-10eb-4d5e-c8ea-cae6de4ac7d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표준어 토큰 개수 : 23368\n",
            "제주어 토큰 개수 : 25094\n"
          ]
        }
      ],
      "source": [
        "BUFFER_SIZE = len(std_tensor_train)\n",
        "BATCH_SIZE = 128  # Out of Memory 에러 주의\n",
        "steps_per_epoch = len(std_tensor_train) // BATCH_SIZE\n",
        "embedding_size = 1024\n",
        "units = 1024\n",
        "\n",
        "vocab_input_size = len(std_lang.word_index) + 1\n",
        "vocab_target_size = len(jej_lang.word_index) + 1\n",
        "\n",
        "print('표준어 토큰 개수 : {}'.format(vocab_input_size))\n",
        "print('제주어 토큰 개수 : {}'.format(vocab_target_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qaY-DpPRP_GZ"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((std_tensor_train, jej_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBW3oxbP_GZ",
        "outputId": "d25401c5-712d-4ecf-ebbe-7959f1d6a0d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 34) (128, 34)\n"
          ]
        }
      ],
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "\n",
        "print(example_input_batch.shape, example_target_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kSpxd6JJP_GZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw1IztAzP_Ga",
        "outputId": "6ceba92d-0933-4f9d-fc69-7adabffba3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output (batch size, sequence length, units) = (128, 34, 1024)\n",
            "Encoder Hidden state  (batch size, units) = (128, 1024)\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(vocab_input_size, embedding_size, units, BATCH_SIZE)\n",
        "\n",
        "#샘플 입력\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print(f'Encoder output (batch size, sequence length, units) = {sample_output.shape}')\n",
        "print(f'Encoder Hidden state  (batch size, units) = {sample_hidden.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MHU_eC9YP_Ga"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        #query hidden state는 (batch_size, hidden_size)로 구성\n",
        "        #query_with_time_axis는 (batch_size, 1, hidden_size)로 구성\n",
        "        #values는 (batch_size, max_len, hidden_size)로 구성\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        #score는 (batch_size, max_len, units)로 구성\n",
        "        #score를 self.V에 적용하기 때문에 마지막 축에 1을 얻어 (batch_size, max_len, 1)로 구성되게 됨\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "\n",
        "        #attention_weights는 (batch_size, max_len, 1)로 구성\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        #병합 이후 context_vector는 (batch_size, hidden_size)로 구성\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ4AKnIXP_Ga",
        "outputId": "33bead70-a42b-4c13-9c9d-f95fdf52244c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (128, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (128, 34, 1)\n"
          ]
        }
      ],
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(f\"Attention result shape: (batch size, units) {attention_result.shape}\")\n",
        "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ANP6Z5w-P_Ga"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):    # 단어 하나하나 해석 진행\n",
        "        #            hidden (batch_size, units),    enc_output (batch_size, max_length_inp, enc_units)\n",
        "        # =>context_vector (batch_size, enc_units), attention_weights (batch_size, max_length_inp, 1)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # 임베딩 층 통과 후 x는 (batch_size, 1, embedding_dim)로 구성\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        #context vector과 임베딩 결과를 결합한 후 x는 (batch_size, 1, embedding_dim+hidden_size)로 구성\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        #위에서 결합된 벡터를 GRU에 전달\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        #output은 (batch_size*1, hidden_size)로 구성\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        #FC(완전연결층)을 지난 x는 (batch_size, vocab)으로 구성\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAIp4zpRP_Ga",
        "outputId": "da437ecd-6b0e-4896-df57-0007b478b3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (128, 25094)\n"
          ]
        }
      ],
      "source": [
        "decoder = Decoder(vocab_target_size, embedding_size, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "edZnV8PrP_Ga"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_objects = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                             reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_objects(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hWXZih_KP_Ga"
      },
      "outputs": [],
      "source": [
        "#체크포인트(객체 기반 저장)\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"cpkt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "\n",
        "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir,\n",
        "                                     checkpoint_name='model.ckpt',\n",
        "                                     max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gsW9x9lsP_Ga"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([jej_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # enc_output을 디코더에 전달\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)  # teacher forcing\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZubS-nYP_Ga",
        "outputId": "1632c48d-f228-46bc-8605-6a4cf353fcb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.7366\n"
          ]
        }
      ],
      "source": [
        "\n",
        "EPOCHS =90\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1, batch, batch_loss.numpy()))\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "    manager.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3B-Wih1P_Ga"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_jej, max_length_std))\n",
        "\n",
        "    sentence = preprocess_kr(sentence)\n",
        "\n",
        "    inputs = [std_lang.word_index.get(i, 0) for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_std, padding='post')\n",
        "\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([jej_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_jej):\n",
        "        predictions, dec_hidden, attention_weights = decoder(\n",
        "            dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # 나중에 attention 가중치를 시각화하기 위해 저장해두기\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += jej_lang.index_word.get(predicted_id, '') + ' '\n",
        "\n",
        "        if jej_lang.index_word.get(predicted_id) == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # 예측된 id를 모델에 다시 feeding\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHd276ytP_Gb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    cax = ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 16}\n",
        "    ax.set_xticklabels([''] + sentence.split(' '), fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence.split(' '), fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    fig.colorbar(cax)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZIhiHYUP_Gb"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print(\"Input : %s\" % (sentence))\n",
        "    print(\"Translation : {}\".format(result))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo-cDFAnP_Gb"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kzH1MzUP_Gb"
      },
      "outputs": [],
      "source": [
        "val_df = pd.read_csv('/content/drive/MyDrive/colab/Jeju/val_df.csv', index_col=0)\n",
        "val_sample = val_df.sample(10)\n",
        "val_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWn-jK4LP_Gb"
      },
      "outputs": [],
      "source": [
        "for idx, i in enumerate(val_sample['표준어'].values):\n",
        "    try:\n",
        "        translate(u'{}'.format(i))\n",
        "        print(\"Intended Output : %s\" % (val_sample.iloc[idx,1]))\n",
        "        print(\" \")\n",
        "    except:\n",
        "        print(i, '=> 데이터셋에 없는 단어 포함')\n",
        "        print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgezk9mwP_Gb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}