{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, re, jieba\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1089907746353116705\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print('GPU Running')\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kr(w):\n",
    "    w = re.sub(r\"([?'!¿\\-·\\\"])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', r\" \", w)\n",
    "    w = re.sub(r\"\\,(?=[0-9])\", r\"\", w)\n",
    "    w = w[:-1].strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일을 읽고 필요한 데이터를 추출하는 함수\n",
    "def extract_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    standard_forms = []\n",
    "    dialect_forms = []\n",
    "    \n",
    "    for utterance in data['utterance']:\n",
    "        standard_forms.append(utterance['standard_form'])\n",
    "        dialect_forms.append(utterance['dialect_form'])\n",
    "    \n",
    "    return standard_forms, dialect_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준어와 제주어 데이터를 전처리하고 크기를 제한하는 함수\n",
    "def preprocess(path, num_data):\n",
    "    files = glob.glob(os.path.join(path, '*.json'))\n",
    "    std, jej = [], []  # 빈 리스트 초기화\n",
    "    \n",
    "    for f in files:\n",
    "        std_forms, dial_forms = extract_data_from_json(f)\n",
    "        std.extend(std_forms)\n",
    "        jej.extend(dial_forms)\n",
    "    \n",
    "    std_series = pd.Series(std)\n",
    "    jej_series = pd.Series(jej)\n",
    "    \n",
    "    df = pd.concat([std_series, jej_series], axis=1)\n",
    "    df.columns = ['표준어', '제주어']\n",
    "    \n",
    "    df['표준어'] = df['표준어'].apply(preprocess_kr)\n",
    "    df['제주어'] = df['제주어'].apply(preprocess_kr)\n",
    "    \n",
    "    df = df.sample(num_data, random_state=2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    tensor = tokenizer.texts_to_sequences(texts)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 토크나이즈하고 텐서로 변환하는 함수\n",
    "def tokenize_dataset(path, num_data):\n",
    "    df = preprocess(path, num_data)\n",
    "    \n",
    "    std_tensor, std_tokenizer = tokenize(df['표준어'].values)\n",
    "    jej_tensor, jej_tokenizer = tokenize(df['제주어'].values)\n",
    "    \n",
    "    return std_tensor, jej_tensor, std_tokenizer, jej_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표준어 tensor 최장 길이 : 26\n",
      "제주어 tensor 최장 길이 : 26\n"
     ]
    }
   ],
   "source": [
    "# 설정한 경로와 데이터 크기 제한\n",
    "num_data = 3000  # 이 이상이면 OOM error 발생...\n",
    "path = os.getcwd() + '\\\\Training'\n",
    "\n",
    "# 약 5분 소요\n",
    "std_tensor, jej_tensor, std_lang, jej_lang = tokenize_dataset(path, num_data)\n",
    "\n",
    "# 입력 텐서와 타겟 텐서의 최대 길이 계산\n",
    "max_length_std = std_tensor.shape[1]\n",
    "max_length_jej = jej_tensor.shape[1]\n",
    "\n",
    "print('표준어 tensor 최장 길이 : {}'.format(max_length_std))\n",
    "print('제주어 tensor 최장 길이 : {}'.format(max_length_jej))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터셋 크기 : 2400, 2400\n",
      "검증 데이터셋 크기 : 600, 600\n"
     ]
    }
   ],
   "source": [
    "# 표준어와 제주어 텐서를 훈련 데이터셋과 검증 데이터셋으로 나누기\n",
    "std_tensor_train, std_tensor_val, jej_tensor_train, jej_tensor_val = train_test_split(std_tensor, jej_tensor, test_size=0.2)\n",
    "\n",
    "print('훈련 데이터셋 크기 : {}, {}'.format(len(std_tensor_train), len(jej_tensor_train)))\n",
    "print('검증 데이터셋 크기 : {}, {}'.format(len(std_tensor_val), len(jej_tensor_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표준어 index ----> token\n",
      "           1 ----> <start>\n",
      "         146 ----> 삼\n",
      "        4305 ----> 사일전에\n",
      "        4306 ----> 시내에서\n",
      "        1282 ----> 카\n",
      "           2 ----> <end>\n",
      "\n",
      "제주어 index ----> token\n",
      "           1 ----> <start>\n",
      "         124 ----> 삼\n",
      "        4497 ----> 사일전에\n",
      "        4498 ----> 시내에서\n",
      "        1261 ----> 카\n",
      "           2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(tokenizer, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(\"%12d ----> %s\" % (t, tokenizer.index_word[t]))\n",
    "\n",
    "print('표준어 index ----> token')\n",
    "convert(std_lang, std_tensor_train[0])\n",
    "print()\n",
    "print('제주어 index ----> token')\n",
    "convert(jej_lang, jej_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표준어 토큰 개수 : 6610\n",
      "제주어 토큰 개수 : 6960\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(std_tensor_train)\n",
    "BATCH_SIZE = 4  # Out of Memory 에러 주의\n",
    "steps_per_epoch = len(std_tensor_train) // BATCH_SIZE\n",
    "embedding_size = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_input_size = len(std_lang.word_index) + 1\n",
    "vocab_target_size = len(jej_lang.word_index) + 1\n",
    "\n",
    "print('표준어 토큰 개수 : {}'.format(vocab_input_size))\n",
    "print('제주어 토큰 개수 : {}'.format(vocab_target_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((std_tensor_train, jej_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 26) (4, 26)\n"
     ]
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "print(example_input_batch.shape, example_target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output (batch size, sequence length, units) = (4, 26, 1024)\n",
      "Encoder Hidden state  (batch size, units) = (4, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_input_size, embedding_size, units, BATCH_SIZE)\n",
    "\n",
    "#샘플 입력\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print(f'Encoder output (batch size, sequence length, units) = {sample_output.shape}')   \n",
    "print(f'Encoder Hidden state  (batch size, units) = {sample_hidden.shape}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        #query hidden state는 (batch_size, hidden_size)로 구성\n",
    "        #query_with_time_axis는 (batch_size, 1, hidden_size)로 구성\n",
    "        #values는 (batch_size, max_len, hidden_size)로 구성\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        #score는 (batch_size, max_len, units)로 구성\n",
    "        #score를 self.V에 적용하기 때문에 마지막 축에 1을 얻어 (batch_size, max_len, 1)로 구성되게 됨\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "        #attention_weights는 (batch_size, max_len, 1)로 구성\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        #병합 이후 context_vector는 (batch_size, hidden_size)로 구성\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (4, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (4, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(f\"Attention result shape: (batch size, units) {attention_result.shape}\")\n",
    "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):    # 단어 하나하나 해석 진행 \n",
    "        #            hidden (batch_size, units),    enc_output (batch_size, max_length_inp, enc_units)\n",
    "        # =>context_vector (batch_size, enc_units), attention_weights (batch_size, max_length_inp, 1)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # 임베딩 층 통과 후 x는 (batch_size, 1, embedding_dim)로 구성\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #context vector과 임베딩 결과를 결합한 후 x는 (batch_size, 1, embedding_dim+hidden_size)로 구성\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        #위에서 결합된 벡터를 GRU에 전달\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        #output은 (batch_size*1, hidden_size)로 구성\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        #FC(완전연결층)을 지난 x는 (batch_size, vocab)으로 구성\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (4, 6960)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_target_size, embedding_size, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_objects = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                             reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_objects(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#체크포인트(객체 기반 저장)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"cpkt\")\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, \n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir,\n",
    "                                     checkpoint_name='model.ckpt',\n",
    "                                     max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([jej_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # enc_output을 디코더에 전달\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)  # teacher forcing\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0419\n",
      "Epoch 1 Batch 100 Loss 0.9824\n",
      "Epoch 1 Batch 200 Loss 1.6082\n",
      "Epoch 1 Batch 300 Loss 2.3268\n",
      "Epoch 1 Batch 400 Loss 1.2970\n",
      "Epoch 1 Batch 500 Loss 1.6157\n",
      "Epoch 1 Loss 1.7871\n",
      "Time taken for 1 epoch 389.8684298992157 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5070\n",
      "Epoch 2 Batch 100 Loss 2.6486\n",
      "Epoch 2 Batch 200 Loss 0.9572\n",
      "Epoch 2 Batch 300 Loss 1.5941\n",
      "Epoch 2 Batch 400 Loss 0.6741\n",
      "Epoch 2 Batch 500 Loss 2.1042\n",
      "Epoch 2 Loss 1.6007\n",
      "Time taken for 1 epoch 372.1045353412628 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.9647\n",
      "Epoch 3 Batch 100 Loss 1.3118\n",
      "Epoch 3 Batch 200 Loss 0.9728\n",
      "Epoch 3 Batch 300 Loss 1.8008\n",
      "Epoch 3 Batch 400 Loss 0.8979\n",
      "Epoch 3 Batch 500 Loss 1.4724\n",
      "Epoch 3 Loss 1.4097\n",
      "Time taken for 1 epoch 375.95343351364136 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.5231\n",
      "Epoch 4 Batch 100 Loss 1.2778\n",
      "Epoch 4 Batch 200 Loss 0.6188\n",
      "Epoch 4 Batch 300 Loss 1.5558\n",
      "Epoch 4 Batch 400 Loss 1.3813\n",
      "Epoch 4 Batch 500 Loss 1.4697\n",
      "Epoch 4 Loss 1.2222\n",
      "Time taken for 1 epoch 377.79545307159424 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7864\n",
      "Epoch 5 Batch 100 Loss 0.7814\n",
      "Epoch 5 Batch 200 Loss 1.3211\n",
      "Epoch 5 Batch 300 Loss 1.2843\n",
      "Epoch 5 Batch 400 Loss 0.5607\n",
      "Epoch 5 Batch 500 Loss 1.2461\n",
      "Epoch 5 Loss 1.1120\n",
      "Time taken for 1 epoch 572.2046277523041 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7210\n",
      "Epoch 6 Batch 100 Loss 1.1543\n",
      "Epoch 6 Batch 200 Loss 0.3135\n",
      "Epoch 6 Batch 300 Loss 0.8793\n",
      "Epoch 6 Batch 400 Loss 0.7478\n",
      "Epoch 6 Batch 500 Loss 0.7324\n",
      "Epoch 6 Loss 0.9237\n",
      "Time taken for 1 epoch 583.0439488887787 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.0160\n",
      "Epoch 7 Batch 100 Loss 1.0038\n",
      "Epoch 7 Batch 200 Loss 0.8162\n",
      "Epoch 7 Batch 300 Loss 0.2484\n",
      "Epoch 7 Batch 400 Loss 0.8184\n",
      "Epoch 7 Batch 500 Loss 0.4587\n",
      "Epoch 7 Loss 0.7865\n",
      "Time taken for 1 epoch 603.8625469207764 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.4324\n",
      "Epoch 8 Batch 100 Loss 1.3213\n",
      "Epoch 8 Batch 200 Loss 0.7265\n",
      "Epoch 8 Batch 300 Loss 0.6990\n",
      "Epoch 8 Batch 400 Loss 0.5026\n",
      "Epoch 8 Batch 500 Loss 0.5396\n",
      "Epoch 8 Loss 0.6108\n",
      "Time taken for 1 epoch 610.4619424343109 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.2207\n",
      "Epoch 9 Batch 100 Loss 0.1835\n",
      "Epoch 9 Batch 200 Loss 0.2854\n",
      "Epoch 9 Batch 300 Loss 0.2183\n",
      "Epoch 9 Batch 400 Loss 0.2379\n",
      "Epoch 9 Batch 500 Loss 0.4721\n",
      "Epoch 9 Loss 0.4685\n",
      "Time taken for 1 epoch 617.1117558479309 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4844\n",
      "Epoch 10 Batch 100 Loss 0.2591\n",
      "Epoch 10 Batch 200 Loss 0.4761\n",
      "Epoch 10 Batch 300 Loss 0.8016\n",
      "Epoch 10 Batch 400 Loss 0.2759\n",
      "Epoch 10 Batch 500 Loss 0.1723\n",
      "Epoch 10 Loss 0.3339\n",
      "Time taken for 1 epoch 619.4460496902466 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.1220\n",
      "Epoch 11 Batch 100 Loss 0.4401\n",
      "Epoch 11 Batch 200 Loss 0.1493\n",
      "Epoch 11 Batch 300 Loss 0.2436\n",
      "Epoch 11 Batch 400 Loss 0.4909\n",
      "Epoch 11 Batch 500 Loss 0.1291\n",
      "Epoch 11 Loss 0.2318\n",
      "Time taken for 1 epoch 519.7084429264069 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0210\n",
      "Epoch 12 Batch 100 Loss 0.1045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f}\".format(epoch+1, batch, batch_loss.numpy()))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch+1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "    manager.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_jej, max_length_std))\n",
    "\n",
    "    sentence = preprocess_kr(sentence)\n",
    "    \n",
    "    inputs = [std_lang.word_index.get(i, 0) for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_std, padding='post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([jej_lang.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_jej):\n",
    "        predictions, dec_hidden, attention_weights = decoder(\n",
    "            dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # 나중에 attention 가중치를 시각화하기 위해 저장해두기\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += jej_lang.index_word.get(predicted_id, '') + ' '\n",
    "\n",
    "        if jej_lang.index_word.get(predicted_id) == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 예측된 id를 모델에 다시 feeding\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cax = ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 16}\n",
    "    ax.set_xticklabels([''] + sentence.split(' '), fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence.split(' '), fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print(\"Input : %s\" % (sentence))\n",
    "    print(\"Translation : {}\".format(result))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x286daa13fc8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>표준어</th>\n",
       "      <th>제주어</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253834</th>\n",
       "      <td>{laughing}</td>\n",
       "      <td>{laughing}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182268</th>\n",
       "      <td>그러니까 여기는</td>\n",
       "      <td>그러니까 여기는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153813</th>\n",
       "      <td>응.</td>\n",
       "      <td>응.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319043</th>\n",
       "      <td>그게 큰일이 아니고 피임 같은 거 없었지 않았잖아</td>\n",
       "      <td>그게 큰일이 아니고 피임 같은 거 없었지 않안게이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243571</th>\n",
       "      <td>걔네도 귤 따서</td>\n",
       "      <td>가이네 미깡 타네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82106</th>\n",
       "      <td>그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르고</td>\n",
       "      <td>그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77858</th>\n",
       "      <td>누구?</td>\n",
       "      <td>누구?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250733</th>\n",
       "      <td>오빠 봐라 (())</td>\n",
       "      <td>오라방 보라 (())</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162596</th>\n",
       "      <td>돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 하는데</td>\n",
       "      <td>돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 허는디</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30430</th>\n",
       "      <td>거기는 진짜 개 맛 있어 껍데기도 서비스로 주고</td>\n",
       "      <td>거기는 진짜 개 맛 인 껍데기도 서비스로 주고</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    표준어                              제주어\n",
       "253834                       {laughing}                       {laughing}\n",
       "182268                         그러니까 여기는                         그러니까 여기는\n",
       "153813                               응.                               응.\n",
       "319043      그게 큰일이 아니고 피임 같은 거 없었지 않았잖아      그게 큰일이 아니고 피임 같은 거 없었지 않안게이\n",
       "243571                         걔네도 귤 따서                        가이네 미깡 타네\n",
       "82106    그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르고   그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르고\n",
       "77858                               누구?                              누구?\n",
       "250733                       오빠 봐라 (())                      오라방 보라 (())\n",
       "162596  돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 하는데  돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 허는디\n",
       "30430        거기는 진짜 개 맛 있어 껍데기도 서비스로 주고        거기는 진짜 개 맛 인 껍데기도 서비스로 주고"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('val_df.csv', index_col=0)\n",
    "val_sample = val_df.sample(10)\n",
    "val_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : <start> {laughing <end>\n",
      "Translation : {laughing <end> \n",
      "Intended Output : {laughing}\n",
      " \n",
      "Input : <start> 그러니까 여기 <end>\n",
      "Translation : 종류가 여기 <end> \n",
      "Intended Output : 그러니까 여기는\n",
      " \n",
      "Input : <start> 응 <end>\n",
      "Translation : 응 <end> \n",
      "Intended Output : 응.\n",
      " \n",
      "Input : <start> 그게 큰일이 아니고 피임 같은 거 없었지 않았잖 <end>\n",
      "Translation : 그게 <end> \n",
      "Intended Output : 그게 큰일이 아니고 피임 같은 거 없었지 않안게이\n",
      " \n",
      "Input : <start> 걔네도 귤 따 <end>\n",
      "Translation : <end> \n",
      "Intended Output : 가이네 미깡 타네\n",
      " \n",
      "Input : <start> 그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르 <end>\n",
      "Translation : 그냥 가이 그냥 (()) 멜 이게 이게 <end> \n",
      "Intended Output : 그냥 맞아 걍 걍 대한항공 아무 데나 이렇게 띡 누르고\n",
      " \n",
      "Input : <start> 누구 ? <end>\n",
      "Translation : 누구 ? <end> \n",
      "Intended Output : 누구?\n",
      " \n",
      "Input : <start> 오빠 봐라 (() <end>\n",
      "Translation : 바닥이 지나서 학교를 (() <end> \n",
      "Intended Output : 오라방 보라 (())\n",
      " \n",
      "Input : <start> 돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 하는 <end>\n",
      "Translation : 해녀가 안 오는 <end> \n",
      "Intended Output : 돈도 안 쓰고 밥하고 깨끗하게 청소하고 이런 거는 허는디\n",
      " \n",
      "Input : <start> 거기는 진짜 개 맛 있어 껍데기도 서비스로 주 <end>\n",
      "Translation : 그디는 진짜 개라그네 개라그네 안가고 있 <end> \n",
      "Intended Output : 거기는 진짜 개 맛 인 껍데기도 서비스로 주고\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(val_sample['표준어'].values):\n",
    "    try:\n",
    "        translate(u'{}'.format(i))\n",
    "        print(\"Intended Output : %s\" % (val_sample.iloc[idx,1]))\n",
    "        print(\" \")\n",
    "    except:\n",
    "        print(i, '=> 데이터셋에 없는 단어 포함')\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
