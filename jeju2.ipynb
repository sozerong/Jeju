{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, re, jieba\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model, save_model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_kr(w):\n",
    "    w = re.sub(r\"([?'!¿\\-·\\\"])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[ |ㄱ-ㅎ|ㅏ-ㅣ]+', r\" \", w)\n",
    "    w = re.sub(r\"\\,(?=[0-9])\", r\"\", w)\n",
    "    w = w[:-1].strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일을 읽고 필요한 데이터를 추출하는 함수\n",
    "def extract_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    standard_forms = []\n",
    "    dialect_forms = []\n",
    "    \n",
    "    for utterance in data['utterance']:\n",
    "        standard_forms.append(utterance['standard_form'])\n",
    "        dialect_forms.append(utterance['dialect_form'])\n",
    "    \n",
    "    return standard_forms, dialect_forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표준어와 제주어 데이터를 전처리하고 크기를 제한하는 함수\n",
    "def preprocess(path, num_data):\n",
    "    files = glob.glob(os.path.join(path, '*.json'))\n",
    "    std, jej = [], []  # 빈 리스트 초기화\n",
    "    \n",
    "    for f in files:\n",
    "        std_forms, dial_forms = extract_data_from_json(f)\n",
    "        std.extend(std_forms)\n",
    "        jej.extend(dial_forms)\n",
    "    \n",
    "    std_series = pd.Series(std)\n",
    "    jej_series = pd.Series(jej)\n",
    "    \n",
    "    df = pd.concat([std_series, jej_series], axis=1)\n",
    "    df.columns = ['표준어', '제주어']\n",
    "    \n",
    "    df['표준어'] = df['표준어'].apply(preprocess_kr)\n",
    "    df['제주어'] = df['제주어'].apply(preprocess_kr)\n",
    "    \n",
    "    df = df.sample(num_data, random_state=2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(train_df):\n",
    "    std_vocab, jej_vocab = set(), set()\n",
    "\n",
    "    for line in train_df['표준어']:\n",
    "        for c in line:\n",
    "            std_vocab.add(c)\n",
    "\n",
    "    for line in train_df['제주어']:\n",
    "        for c in line:\n",
    "            jej_vocab.add(c)\n",
    "    \n",
    "    std_vocab.add('<start>')\n",
    "    std_vocab.add('<end>')\n",
    "    jej_vocab.add('<start>')\n",
    "    jej_vocab.add('<end>')\n",
    "    \n",
    "    std_vocab_size = len(std_vocab) + 1\n",
    "    jej_vocab_size = len(jej_vocab) + 1\n",
    "    \n",
    "    std_vocab = sorted(list(std_vocab))\n",
    "    jej_vocab = sorted(list(jej_vocab))\n",
    "    \n",
    "    std_to_index = {c: i+1 for i, c in enumerate(std_vocab)}\n",
    "    jej_to_index = {c: i+1 for i, c in enumerate(jej_vocab)}\n",
    "    \n",
    "    encoder_input = []\n",
    "    for line in train_df['표준어']:\n",
    "        encoded_line = [std_to_index[c] for c in line]\n",
    "        encoder_input.append(encoded_line)\n",
    "        \n",
    "    decoder_input = []\n",
    "    for line in train_df['제주어']:\n",
    "        encoded_line = [jej_to_index[c] for c in line]\n",
    "        decoder_input.append(encoded_line)\n",
    "        \n",
    "    decoder_target = []\n",
    "    for line in train_df['제주어']:\n",
    "        encoded_line = [jej_to_index[c] for c in line[1:]]\n",
    "        decoder_target.append(encoded_line)\n",
    "    \n",
    "    max_len_std = max(len(seq) for seq in encoder_input)\n",
    "    max_len_jej = max(len(seq) for seq in decoder_input)\n",
    "    \n",
    "    encoder_input = pad_sequences(encoder_input, maxlen=max_len_std, padding='post')\n",
    "    decoder_input = pad_sequences(decoder_input, maxlen=max_len_jej, padding='post')\n",
    "    decoder_target = pad_sequences(decoder_target, maxlen=max_len_jej, padding='post')\n",
    "    \n",
    "    encoder_input = to_categorical(encoder_input, num_classes=std_vocab_size)\n",
    "    decoder_input = to_categorical(decoder_input, num_classes=jej_vocab_size)\n",
    "    decoder_target = to_categorical(decoder_target, num_classes=jej_vocab_size)\n",
    "    \n",
    "    return encoder_input, decoder_input, decoder_target, std_vocab_size, jej_vocab_size, std_to_index, jej_to_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_df[:10000]\n",
    "\n",
    "encoder_input, decoder_input, decoder_ko, std_vocab_size, jej_vocab_size, std_to_index, jej_to_index = tokenize(df)\n",
    "\n",
    "# 표준어 인코딩\n",
    "tmp_dict = dict((i, c) for c, i in std_to_index.items())\n",
    "\n",
    "for i in tmp_dict:\n",
    "    try:\n",
    "        tmp_dict[i] = tmp_dict[i].encode('utf-8')  # 'EUC_CN'을 'utf-8'로 변경\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "index_to_std = dict((i, c) for c, i in tmp_dict.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, None, 1199)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " decoder_input (InputLayer)     [(None, None, 1246)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoderLSTM (LSTM)             [(None, 512),        3506176     ['encoder_input[0][0]']          \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " decoderLSTM (LSTM)             [(None, None, 512),  3602432     ['decoder_input[0][0]',          \n",
      "                                 (None, 512),                     'encoderLSTM[0][1]',            \n",
      "                                 (None, 512)]                     'encoderLSTM[0][2]']            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, None, 1246)   639198      ['decoderLSTM[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,747,806\n",
      "Trainable params: 7,747,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 트레이닝 시 이전 상태의 실제값을 현재 상태의 디코더 입력으로 해야 함 (예측값으로 하면 안 됨)\n",
    "encoder_inputs = Input(shape=(None, std_vocab_size), name='encoder_input')\n",
    "decoder_inputs = Input(shape=(None, jej_vocab_size), name='decoder_input')\n",
    "\n",
    "# 인코더 LSTM 셀\n",
    "encoderLSTM = LSTM(units=512, return_state=True, name='encoderLSTM')  # return_state : 인코더의 마지막 상태 정보를 디코더의 입력 상태 정보로 전달\n",
    "decoderLSTM = LSTM(units=512, return_sequences=True, return_state=True, name='decoderLSTM')\n",
    "\n",
    "# 인코더 LSTM셀의 입력 정의\n",
    "encoder_outputs, stateH, stateC = encoderLSTM(encoder_inputs)  # _, 히든 상태(위), 셀 상태(오른쪽)\n",
    "encoder_state = [stateH, stateC]  # 컨텍스트 벡터\n",
    "\n",
    "decoder_output, _, _ = decoderLSTM(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_softmax = Dense(jej_vocab_size, activation=\"softmax\")\n",
    "decoder_output = decoder_softmax(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_output)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 203s 3s/step - loss: 1.7894 - val_loss: 1.2347\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 1.1801 - val_loss: 1.1309\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.9730 - val_loss: 0.8952\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 211s 3s/step - loss: 0.8537 - val_loss: 0.8072\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.7814 - val_loss: 0.7522\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.7373 - val_loss: 0.7174\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.7039 - val_loss: 0.6884\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.6757 - val_loss: 0.6636\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.6529 - val_loss: 0.6433\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.6346 - val_loss: 0.6285\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.6196 - val_loss: 0.6185\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.6074 - val_loss: 0.6061\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.5964 - val_loss: 0.5985\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 1.6138 - val_loss: 1.0385\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.8033 - val_loss: 0.6462\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.6243 - val_loss: 0.6130\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.6012 - val_loss: 0.5999\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.5894 - val_loss: 0.5942\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.5811 - val_loss: 0.5884\n",
      "Epoch 20/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5742 - val_loss: 0.5841\n",
      "Epoch 21/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.5681 - val_loss: 0.5816\n",
      "Epoch 22/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.5627 - val_loss: 0.5771\n",
      "Epoch 23/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5575 - val_loss: 0.5748\n",
      "Epoch 24/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.5529 - val_loss: 0.5731\n",
      "Epoch 25/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5487 - val_loss: 0.5709\n",
      "Epoch 26/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.5442 - val_loss: 0.5686\n",
      "Epoch 27/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.5399 - val_loss: 0.5670\n",
      "Epoch 28/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.5360 - val_loss: 0.5653\n",
      "Epoch 29/50\n",
      "63/63 [==============================] - 219s 3s/step - loss: 0.5323 - val_loss: 0.5630\n",
      "Epoch 30/50\n",
      "63/63 [==============================] - 217s 3s/step - loss: 0.5285 - val_loss: 0.5625\n",
      "Epoch 31/50\n",
      "63/63 [==============================] - 218s 3s/step - loss: 0.5249 - val_loss: 0.5614\n",
      "Epoch 32/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5216 - val_loss: 0.5582\n",
      "Epoch 33/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.5184 - val_loss: 0.5598\n",
      "Epoch 34/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.5143 - val_loss: 0.5584\n",
      "Epoch 35/50\n",
      "63/63 [==============================] - 216s 3s/step - loss: 0.5113 - val_loss: 0.5560\n",
      "Epoch 36/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5079 - val_loss: 0.5577\n",
      "Epoch 37/50\n",
      "63/63 [==============================] - 215s 3s/step - loss: 0.5046 - val_loss: 0.5549\n",
      "Epoch 38/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.5012 - val_loss: 0.5536\n",
      "Epoch 39/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.4980 - val_loss: 0.5550\n",
      "Epoch 40/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.4945 - val_loss: 0.5543\n",
      "Epoch 41/50\n",
      "63/63 [==============================] - 211s 3s/step - loss: 0.4913 - val_loss: 0.5543\n",
      "Epoch 42/50\n",
      "63/63 [==============================] - 211s 3s/step - loss: 0.4883 - val_loss: 0.5530\n",
      "Epoch 43/50\n",
      "63/63 [==============================] - 211s 3s/step - loss: 0.4855 - val_loss: 0.5535\n",
      "Epoch 44/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.4820 - val_loss: 0.5524\n",
      "Epoch 45/50\n",
      "63/63 [==============================] - 210s 3s/step - loss: 0.4787 - val_loss: 0.5552\n",
      "Epoch 46/50\n",
      "63/63 [==============================] - 210s 3s/step - loss: 0.4753 - val_loss: 0.5513\n",
      "Epoch 47/50\n",
      "63/63 [==============================] - 212s 3s/step - loss: 0.4720 - val_loss: 0.5538\n",
      "Epoch 48/50\n",
      "63/63 [==============================] - 211s 3s/step - loss: 0.4689 - val_loss: 0.5530\n",
      "Epoch 49/50\n",
      "63/63 [==============================] - 213s 3s/step - loss: 0.4658 - val_loss: 0.5511\n",
      "Epoch 50/50\n",
      "63/63 [==============================] - 214s 3s/step - loss: 0.4623 - val_loss: 0.5515\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(\n",
    "    x=[encoder_input, decoder_input],\n",
    "    y=decoder_ko,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# 모델 저장\n",
    "save_model(model, 'std_to_jej.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13936\\3829208187.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mencoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_ko\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_vocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjej_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'std_to_jej.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13936\\3560286758.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstd_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjej_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjej_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mencoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mdecoder_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjej_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Adjust based on your needs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 데이터프레임을 2500개씩 분할하여 모델을 훈련\n",
    "for i in range(1, len(train_df) // 5000):\n",
    "    df = train_df[i * 5000:(i + 1) * 5000]\n",
    "    \n",
    "    encoder_input, decoder_input, decoder_ko, std_vocab_size, jej_vocab_size = tokenize(df)\n",
    "\n",
    "    model = load_model('std_to_jej.h5')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=[encoder_input, decoder_input], y=decoder_ko, batch_size=64, epochs=3, validation_split=0.2, callbacks=[early_stopping])\n",
    "    save_model(model, 'std_to_jej.h5', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 모델 정의\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_state)\n",
    "\n",
    "# 인덱스와 단어의 매핑을 변경\n",
    "std_to_index = dict((i, c) for c, i in std_to_index.items())\n",
    "jej_to_index = dict((i, c) for c, i in jej_to_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, None, 13)]        0         \n",
      "                                                                 \n",
      " encoderLSTM (LSTM)          [(None, 1024),            4251648   \n",
      "                              (None, 1024),                      \n",
      "                              (None, 1024)]                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,251,648\n",
      "Trainable params: 4,251,648\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더\n",
    "decoder_state_input_hidden = Input(shape=(1024,))\n",
    "decoder_state_input_cell = Input(shape=(1024,))\n",
    "decoder_state_input = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "\n",
    "decoder_output, state_hidden, state_cell = decoderLSTM(decoder_inputs, initial_state = decoder_state_input)\n",
    "decoder_state = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_softmax(decoder_output)\n",
    "\n",
    "decoder_model = Model(inputs=[decoder_inputs]+decoder_state_input, outputs=[decoder_output]+decoder_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_input (InputLayer)     [(None, None, 13)]   0           []                               \n",
      "                                                                                                  \n",
      " input_17 (InputLayer)          [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 1024)]       0           []                               \n",
      "                                                                                                  \n",
      " decoderLSTM (LSTM)             [(None, None, 1024)  4251648     ['decoder_input[0][0]',          \n",
      "                                , (None, 1024),                   'input_17[0][0]',               \n",
      "                                 (None, 1024)]                    'input_18[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,251,648\n",
      "Trainable params: 4,251,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(input_seq): \n",
    "    state_value = encoder_model.predict(input_seq)\n",
    "    print('encoder_model의 예상 state_value :', np.shape(state_value))\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, jej_vocab_size))  #(1, 1, 제주어 어휘 크기)\n",
    "    target_seq[0, 0, jej_to_index['<start>']] = 1  # 원핫인코딩\n",
    "    \n",
    "    stop = False\n",
    "    decoded_sent = \"\"\n",
    "    while not stop:  # \"<end>\" 문자를 만날 때까지 반복\n",
    "        \n",
    "        output, state_hidden, state_cell = decoder_model.predict([target_seq, state_value[0], state_value[1]])\n",
    "        # 예측값을 제주어 문자로 변환\n",
    "        token_index = np.argmax(output[0, -1, :]) \n",
    "        pred_char = index_to_jej[token_index]\n",
    "        \n",
    "        # 현시점 예측문자가 예측문장에 추가\n",
    "        decoded_sent += pred_char\n",
    "        \n",
    "        if (pred_char == \"<end>\" or len(decoded_sent) > 373):\n",
    "            stop = True\n",
    "            \n",
    "        # 현시점 예측결과가 다음 시점에 입력으로 \n",
    "        target_seq = np.zeros((1, 1, jej_vocab_size))\n",
    "        target_seq[0, 0, token_index] = 1\n",
    "        \n",
    "        # 현시점 상태를 다음 시점 상태로 사용\n",
    "        state_value = [h, c]\n",
    "    \n",
    "    return decoded_sent  # 번역 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "encoder_model의 예상 state_value : (2, 1, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 217, in assert_input_compatibility\n        f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Exception encountered when calling layer 'model_23' (type Functional).\n    \n    Layer \"decoderLSTM\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 13) dtype=float32>]\n    \n    Call arguments received by layer 'model_23' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 1, 13), dtype=float32)', 'tf.Tensor(shape=(None, 1024), dtype=float32)', 'tf.Tensor(shape=(None, 1024), dtype=float32)')\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13936\\1079140498.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# (1, 117, 표준어 어휘 크기)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdecoded_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"입력문장:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'표준어'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13936\\3687130975.py\u001b[0m in \u001b[0;36mdecode_seq\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# \"<end>\" 문자를 만날 때까지 반복\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m# 예측값을 제주어 문자로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtoken_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\anaconda3\\envs\\trans\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 217, in assert_input_compatibility\n        f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Exception encountered when calling layer 'model_23' (type Functional).\n    \n    Layer \"decoderLSTM\" expects 3 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 13) dtype=float32>]\n    \n    Call arguments received by layer 'model_23' (type Functional):\n      • inputs=('tf.Tensor(shape=(None, 1, 13), dtype=float32)', 'tf.Tensor(shape=(None, 1024), dtype=float32)', 'tf.Tensor(shape=(None, 1024), dtype=float32)')\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [1, 50, 100, 200, 300]:\n",
    "    input_seq = encoder_input[seq_index:seq_index+1]  # (1, 117, 표준어 어휘 크기)\n",
    "    decoded_seq = decode_seq(input_seq)\n",
    "    \n",
    "    print(\"입력문장:\", train_df['표준어'][seq_index])\n",
    "    print(\"정답:\", train_df['제주어'][seq_index][1:len(train_df['제주어'][seq_index])-1])  # \"<start>\", \"<end>\" 제거\n",
    "    print(\"번역기:\", decoded_seq[:len(decoded_seq)-1])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "trans"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
